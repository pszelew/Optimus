{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules import VAE\n",
    "from pytorch_transformers import (WEIGHTS_NAME, AdamW, WarmupLinearSchedule,\n",
    "                                  BertConfig, BertForLatentConnector, BertTokenizer,\n",
    "                                  GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer,\n",
    "                                  OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                                  RobertaConfig, RobertaForMaskedLM, RobertaTokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pytorch_transformers.tokenization_utils:Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args():\n",
    "    encoder_model_type = \"bert\"\n",
    "    decoder_model_type = \"gpt2\"\n",
    "    device = \"cuda\"\n",
    "    checkpoint_dir = \"/home/patryk/Studia/PracaMagisterska/optimus/Optimus/output/checkpoint-508523\"\n",
    "    encoder_model_name_or_path=\"bert-base-cased\"\n",
    "    decoder_model_name_or_path=\"gpt2\"\n",
    "    encoder_tokenizer_name = \"\"\n",
    "    decoder_tokenizer_name = \"\"\n",
    "    do_lower_case = True\n",
    "    global_step = 508523\n",
    "    latent_size = 768\n",
    "    block_size = 100\n",
    "    fb_mode = 1\n",
    "    dim_target_kl = 0.5\n",
    "    temperature = 1.0\n",
    "    top_k = 0.0\n",
    "    top_p = 0.9\n",
    "\n",
    "args = Args()\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForLatentConnector, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(args.global_step))\n",
    "output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(args.global_step)) \n",
    "output_full_dir    = os.path.join(args.checkpoint_dir, 'checkpoint-full-{}'.format(args.global_step)) \n",
    "\n",
    "checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "logging.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "\n",
    "# Load a trained Encoder model and vocabulary\n",
    "encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "model_encoder.to(args.device)\n",
    "if args.block_size <= 0:\n",
    "    args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "# Load a trained Decoder model and vocabulary\n",
    "decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "model_decoder.to(args.device)\n",
    "if args.block_size <= 0:\n",
    "    args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "# Load full model\n",
    "checkpoint = torch.load(os.path.join(output_full_dir, 'training.bin'))\n",
    "\n",
    "model_vae = VAE(\n",
    "    model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, args\n",
    ")\n",
    "\n",
    "model_vae.load_state_dict(checkpoint[\"model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vae.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "def sample_sequence_conditional(model, length, context, past=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu', decoder_tokenizer=None):\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "        # for _ in trange(length):\n",
    "            inputs = {'input_ids': generated, 'past': past}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            print(F.softmax(filtered_logits, dim=-1))\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "            if next_token.unsqueeze(0)[0,0].item() == decoder_tokenizer.encode('<EOS>')[0]:\n",
    "                break\n",
    "\n",
    "    return generated\n",
    "\n",
    "def latent_code_from_text(text, tokenizer_encoder, model_vae, args):\n",
    "    tokenized1 = tokenizer_encoder.encode(text)\n",
    "    tokenized1 = [101] + tokenized1 + [102]\n",
    "    coded1 = torch.Tensor([tokenized1])\n",
    "    coded1 =torch.Tensor.long(coded1)\n",
    "    with torch.no_grad():\n",
    "        x0 = coded1\n",
    "        x0 = x0.to(args.device)\n",
    "        pooled_hidden_fea = model_vae.encoder(x0, attention_mask=(x0 > 0).float())[1]\n",
    "        mean, logvar = model_vae.encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "        # latent_z = model_vae.reparameterize(mean, logvar, 1).squeeze(1)\n",
    "        latent_z = mean.squeeze(1)  \n",
    "\n",
    "        coded_length = len(tokenized1)\n",
    "        return latent_z, coded_length\n",
    "\n",
    "def text_from_latent_code(latent_z, model_vae, args, tokenizer_decoder):\n",
    "    past = latent_z\n",
    "    context_tokens = tokenizer_decoder.encode('<BOS>')\n",
    "\n",
    "    length = 128 # maximum length, but not used \n",
    "    out = sample_sequence_conditional(\n",
    "        model=model_vae.decoder,\n",
    "        context=context_tokens,\n",
    "        past=past,\n",
    "        length= length, # Chunyuan: Fix length; or use <EOS> to complete a sentence\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.top_k,\n",
    "        top_p=args.top_p,\n",
    "        device=args.device,\n",
    "        decoder_tokenizer = tokenizer_decoder\n",
    "    )\n",
    "    text_x1 = tokenizer_decoder.decode(out[0,:].tolist(), clean_up_tokenization_spaces=True)\n",
    "    text_x1 = text_x1.split()[1:-1]\n",
    "    text_x1 = ' '.join(text_x1)\n",
    "    return text_x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"place is a place where you can.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_z = latent_code_from_text(input_text, tokenizer_encoder, model_vae, args)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_latent_code(latent_z, model_vae, args, tokenizer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a62dc5c4e15874d230a4f396bb129d37f109e576af212db1e92385126337577"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
